{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "synonym_test_dataset = pd.read_csv('A2-DataSet/synonym.csv')\n",
    "\n",
    "print(synonym_test_dataset.head())\n",
    "print(synonym_test_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes up to about 5 minuntes to load\n",
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(question_word, correct_answer, closest_synonym, guess_words, model):\n",
    "    # Check if question word and at least one guess word are in the vocabulary\n",
    "    if question_word in model.key_to_index and any(word in model.key_to_index for word in guess_words):\n",
    "        # Check if the closest guess is correct\n",
    "        if closest_synonym is not None and closest_synonym == correct_answer:\n",
    "            return \"correct\"\n",
    "        else:\n",
    "            return \"wrong\"\n",
    "    else:\n",
    "        return \"guess\"\n",
    "\n",
    "def closest_synonym(query, list_of_guess_words, model):\n",
    "    closest_synonym = random.choice(list_of_guess_words) if list_of_guess_words else None\n",
    "    max_similarity = -1\n",
    "\n",
    "    # Check if the query word is in the model's vocabulary\n",
    "    if query not in model.key_to_index:\n",
    "        print(f\"'{query}' is not in the vocabulary.\")\n",
    "        # Return a random guess word if the query is not in the vocabulary\n",
    "        return closest_synonym\n",
    "\n",
    "    for guess_word in list_of_guess_words:\n",
    "        # Check if the guess word is in the model's vocabulary\n",
    "        if guess_word in model.key_to_index:\n",
    "            try:\n",
    "                sim_score = model.similarity(query, guess_word)\n",
    "                if sim_score > max_similarity:\n",
    "                    closest_synonym = guess_word\n",
    "                    max_similarity = sim_score\n",
    "            except KeyError:\n",
    "                # Handle the error if the word is not in the model's vocabulary\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"'{guess_word}' is not in the vocabulary.\")\n",
    "            # Return a random guess word if the query is not in the vocabulary\n",
    "            return closest_synonym\n",
    "\n",
    "    return closest_synonym\n",
    "\n",
    "\n",
    "# Function to proSV file and apply the closest_synonym function\n",
    "def process_csv(file_path, model_name, model):\n",
    "    \n",
    "    # Size of the Vocabulary\n",
    "    vocab_size = len(model.key_to_index)\n",
    "\n",
    "    question_words = []\n",
    "    answer_words = []\n",
    "    guess_words = []\n",
    "    labels = []\n",
    "    C = 0\n",
    "    V = 0\n",
    "    \n",
    "    # Read the CSV file into a Pandas DataFrame, skipping the first row\n",
    "    synonym_test_dataset = pd.read_csv(file_path)\n",
    "\n",
    "    # Process each row in the DataFrame\n",
    "    for index, row in synonym_test_dataset.iterrows():\n",
    "        # Split the row into words based on comma\n",
    "        words = row.to_list()\n",
    "        #print(words)\n",
    "\n",
    "        # Store the first word in 'query' and the rest in 'list_of_guess_words'\n",
    "        query = words[0]\n",
    "        #print(query)\n",
    "        answer = words[1]\n",
    "        #print(answer)\n",
    "        list_of_guess_words = words[2:]\n",
    "        #print(list_of_guess_words)\n",
    "\n",
    "        # Call the 'closest_synonym' function and store the result\n",
    "        result = closest_synonym(query, list_of_guess_words, model)\n",
    "        \n",
    "        question_words.append(query)\n",
    "        answer_words.append(answer)\n",
    "        guess_words.append(result)\n",
    "        label = assign_label(query, answer, result, list_of_guess_words, model)\n",
    "        labels.append(label)\n",
    "        \n",
    "        if label == 'correct':\n",
    "            C += 1\n",
    "        if label != 'guess':\n",
    "            V += 1\n",
    "    \n",
    "    if V == 0:\n",
    "        accuracy = 0\n",
    "    else: \n",
    "        accuracy = C/V\n",
    "    \n",
    "    results_df = pd.DataFrame({'question_word': question_words, 'answer_word': answer_words, 'guess_word': guess_words, 'label': labels})    \n",
    "    results_df.to_csv(f\"{model_name}-details.csv\", index=False)\n",
    "    \n",
    "    analysis_df = pd.DataFrame({'model_name': [model_name], 'vocab_size': [vocab_size], 'C': [C], 'V': [V], 'accuracy': accuracy})    \n",
    "    analysis_df.to_csv('analysis.csv', mode='a', index=False, header=not pd.io.common.file_exists('analysis.csv'))\n",
    "    \n",
    "    print(results_df)\n",
    "    \n",
    "file_path = 'A2-DataSet/synonym.csv' \n",
    "processed_results = process_csv(file_path, 'word2vec-google-news-300', w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we got the corpus names from the offical gensim GitHub page (https://github.com/piskvorky/gensim-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 3 minuntes to load\n",
    "gigaword_model = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 8 minuntes to load\n",
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 5 minuntes to load\n",
    "glove_twitter_200_model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 2 minuntes to load\n",
    "glove_twitter_25_model = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'A2-DataSet/synonym.csv' \n",
    "\n",
    "processed_results = process_csv(file_path, 'glove-wiki-gigaword-300', gigaword_model)\n",
    "processed_results = process_csv(file_path, 'fasttext-wiki-news-subwords-300', fasttext_model)\n",
    "processed_results = process_csv(file_path, 'glove-twitter-200', glove_twitter_200_model)\n",
    "processed_results = process_csv(file_path, 'glove-twitter-25', glove_twitter_25_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_process_docs(file_paths):\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_contents = file.read()\n",
    "            documents.append(process_text(file_contents))\n",
    "    \n",
    "    print(documents)\n",
    "    print(f\"Processed {len(documents)} documents.\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "        # tokenize the text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentences_list = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # tokenize the text\n",
    "            tokens = word_tokenize(sentence)\n",
    "            \n",
    "            # remove punctuation and numbers and convert to lowercase\n",
    "            tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "            \n",
    "            # join the tokens into a single string with each word separated by a space\n",
    "            tokens_string = ' '.join(tokens)\n",
    "            \n",
    "            # return the processed sentence\n",
    "            sentences_list.append(tokens_string)\n",
    "            \n",
    "        \n",
    "        return sentences_list\n",
    "    \n",
    "file_paths = ['Online-Books/Moby_Dick_Or_The_Whale.txt', 'Online-Books/Peter_Pan.txt', 'Online-Books/Pride_and_Prejudice.txt', 'Online-Books/The_Complete_Works_of_William_Shakespeare.txt', 'Online-Books/The_Importance_of_Being_Earnest_A_Trivial_Comedy_for_Serious_People.txt', 'Online-Books/Winnie_the_Pooh.txt']\n",
    "read_and_process_docs(file_paths)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
